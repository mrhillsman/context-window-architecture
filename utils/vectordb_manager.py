import os
import chromadb
import uuid
from chromadb.utils import embedding_functions
from utils.system_prompt import system_prompt_for_rag
from box import Box
import yaml
from pyprojroot import here

with open(here('.config.yml'), 'r') as file:
    config = Box(yaml.safe_load(file))

class VectorDBManager:
    def __init__(self):
        """
        Initializes the VectorDBManager.

        Args:
            config (Config): Configuration object containing parameters for the vector database and models.
        """
        self.cfg = config

        self.embedding_function = embedding_functions.OpenAIEmbeddingFunction(
            api_key=os.getenv("OPENAI_API_KEY"),
            model_name=self.cfg.embedding_model
        )

        self.db_client = chromadb.PersistentClient(
            path=str(self.cfg.vectordb_dir))
        self.db_collection = self.db_client.get_or_create_collection(
            name=self.cfg.collection_name,
            embedding_function=self.embedding_function,
        )

        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.system_prompt = prepare_system_prompt_for_rag_chatbot()

    def update_vector_db(self, msg_pair: dict) -> None:
        """
        Updates the vector database with a new message pair.

        Args:
            msg_pair (dict): A dictionary containing the message pair to be added to the database.

        Returns:
            None
        """
        self.db_collection.add(
            ids=str(uuid.uuid4()),
            documents=[str(msg_pair)]
        )
        print(f"Vector DB updated.")
        return None

    def search_vector_db(self, query: str) -> str:
        """
        Searches the vector database containing the chat history of the user and the chatbot,
        and returns the results.

        Args:
            query (str): The query to be used for the search.

        Returns:
            str: The result generated by the language model based on the search results.
        """
        try:
            print("Pergforming vector search...")
            results = self.db_collection.query(
                query_texts=[query],
                n_results=self.cfg.k
            )
            results = results["documents"][0]
            llm_result = self.prepare_search_result(results, query)
            print("Vector search completed.")
            print("Query: ", query)
            print("Results: ", results)
            print(f"LLM result: {llm_result}")
            return "Function call successful.", llm_result
        except Exception as e:
            return "function call failed.", f"Error: {e}"

    def prepare_search_result(self, search_result: list, query: str) -> str:
        """
        Prepares a structured search result using the language model.

        Args:
            search_result (list[str]): List of search results from the vector database.
            query (str): The original user query.

        Returns:
            str: The response generated by the language model.
        """
        input = f"""
        ## Search Results:\n
        {search_result}

        ## Query:\n
        {query}
        """
        response = self.client.chat.completions.create(
            model=self.cfg.rag_model,
            messages=[{"role": "system", "content": self.system_prompt},
                      {"role": "user", "content": input}]
        )
        return response.choices[0].message.content

    def refresh_vector_db_client(self):
        self.db_client = chromadb.PersistentClient(
            path=str(self.cfg.vectordb_dir))
        self.db_collection = self.db_client.get_or_create_collection(
            name=self.cfg.collection_name,
            embedding_function=self.embedding_function,
        )

        self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))